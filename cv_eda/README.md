## CV EDA

`EDA_CV_Year_project.ipynb` файл разбит на несколько глав:
- Read frames in videos 
	- чтение кадров и их сохранение в папку для дальнейшего анализа, чтобы не приходилось ходить за ними в видеофайлы
- Create model and compute embeds 
	- сохранение модели CLIP в памяти, и расчёт embedding векторов для каждого из кадров
- Cluster analysis 
	- анализ различных кластеров на основе embedding векторов CLIP (анализ описан ниже)
- LSH 
	- тестирование алгоритма базы данных предназначенной для поиска по данным на основе Locality-sensitive hashing
	- более детальный анализ векторного пространства

## Анализ кластеров
Я решил выгрузить кадры с периодичностью один кадр за две секунды
С меньшей частотой кластеры получаются крайне с малым количеством данных, 
С большей частотой данных слишком много и при анализе получаются кластеры с кадрами из одного и того же видео + кадры выгружаются дольше и занимают больше времени. 
Суммарная продолжительность видеофайлов - чуть больше 8 часов, поэтому с частотой описанной выше получается ~30к изображения для анализа
### t-SNE
Для частоты меньше, perplexity приходилось понижать до 5, для выбранной частоты perplexity 50 выдаёт достаточно логичные кластеры

В качестве алгоритма кластеризации выбраны два: KMeans и HDBSCAN - первый как стандарт для анализа, второй выбран по причине качества отделения кластеров

### KMeans
![[Pasted image 20251102064531.png]]
Как видно из scatter-plot кластеры разделены не очень равномерно:
- Большие кластеры делятся слишком сильно (например кластер с центром в (-85; 45) разделен на трети)
- Пары кластеров по краям часто объединяются в пары не смотря на расстояние между кластерами (например два кластера с общим центром в (-45; 100))

Несмотря на это, в зависимости от качества ответов t-SNE кластеры получаются достаточно понятными и их можно часто определить достаточно логичными "названиями":

|      Кудрявые люди      | Люди готовят и показывают это камере |        Животные         |
| :---------------------: | :----------------------------------: | :---------------------: |
| ![[tsne_kmeans/99.png]] |       ![[tsne_kmeans/42.png]]        | ![[tsne_kmeans/44.png]] |

Но при этом можно так же увидеть и кластеры которые были поделены и появляются под разными номерами, например TikTok outro:

|           0            |           31            |           60            |           91            |
| :--------------------: | :---------------------: | :---------------------: | :---------------------: |
| ![[tsne_kmeans/0.png]] | ![[tsne_kmeans/31.png]] | ![[tsne_kmeans/60.png]] | ![[tsne_kmeans/91.png]] |
Несмотря на это, KMeans показывает что даже при сжатии пространства с помощью t-SNE мы всё ещё получаем достаточно хорошие кластеры, которые объединены не только по визуальному сходству, но так же иногда и по тому что находится на кадре.
Больше кластеров можно найти в папке `tsne_kmeans`

### HDBSCAN
Лучше на содержание кластеров может указать HDBSCAN
![[Pasted image 20251102070329.png]]
Получившаяся кластеризация не идеальна, но достаточна для анализа, так например если анализировать в 3d то мы бы получили ещё более детализированные кластеры

|     Рисованные кадры     |           Еда            |       Кадры из игр       |
| :----------------------: | :----------------------: | :----------------------: |
| ![[tsne_hdbscan/5.png]]  | ![[tsne_hdbscan/7.png]]  | ![[tsne_hdbscan/54.png]] |
|          Птицы           |         Собаки*          |          Лошади          |
| ![[tsne_hdbscan/98.png]] | ![[tsne_hdbscan/94.png]] | ![[tsne_hdbscan/91.png]] |
Полученные кластеры можно достаточно хорошо описать и в целом поверх даже t-SNE сжатия можно попробовать обучить модель для как минимум отделения кадров с животными
Больше примеров можно найти в папке `tsne_hdbscan`

## PCA
PCA не показал интересных результатов и отделённый кластер - tiktok outro 
![[Pasted image 20251102071418.png]]

## LSH
У меня была уже реализация LSH для целей написания RAG в одном из домашних заданий по LLM, поэтому я переиспользовал её достаточно простой код в данном анализе
![[Pasted image 20251102071613.png]]
Некоторые значения могут расходится, но это - пример поиска схожих кадров по кадрам уже существовавшим в обучающей выборке, база спокойно находит похожие кадры.
Значения над изображениями указывают на уверенность базы (коэффициент схожести векторов)

Более интересна задача поиска по кадрам которых нет в базе данных, в данных примерах искомый кадры не представлен в обучающей выборке и занимает верхнюю левую позицию

| ![[lsh/Без названия (3).png]] | ![[lsh/Без названия (4).png]] | ![[lsh/Без названия (5).png]]  |
| :---------------------------: | :---------------------------: | :----------------------------: |
| ![[lsh/Без названия (8).png]] | ![[lsh/Без названия (9).png]] | ![[lsh/Без названия (12).png]] |
Как видно из примеров LSH может искать кадры схожие по содержанию и изображённым объектам среди обучающей выборки.
Больше примеров можно найти в папке 'lsh'

Так же стоит упомянуть ещё один тест который был проведён - поиск изображений из обучающей выборки по текстовому запросу.
Этот тест было достаточно проблематично запустить не исследуя гиперпараметры LSH, однако, поиск по тексту работает:
![[Снимок экрана 2025-11-01 083305.png]]

## Итог
Как показывает мой анализ задачу поиска изображений по корпусу видеофайлов выполнить можно, CLIP вполне справляется со своей задачей, потенциально более продвинутые модели, как например DINO могут показывать результат лучше, но это остаётся как задача исследования для дальнейших чекпоинтов

### P.S.
ipynb файл был проверен на повторяемость* получаемых результатов. Код можно запустить  и протестировать работу всех компонентов самостоятельно.
ipynb содержит один баг - первая глава (`Read frames in videos`) - по окончанию выгрузки кадров убивает ядро, и среда jupyter автоматически перезагружается, в случае с локальным исполнением это не приводит к проблемам, но на серверах гугла это может приводить к проблемам