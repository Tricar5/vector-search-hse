{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# NLP Eda\n",
    "\n",
    "\n",
    "Данная часть EDA посвящена анализу смысловой составляющей роликов, которые удалось извлечь расшифровок базы видео.\n",
    " "
   ],
   "id": "74f47c8e11bc58d9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T20:16:07.492180Z",
     "start_time": "2025-11-01T20:16:07.489965Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "ROOT_PATH = pathlib.Path().resolve().parent\n",
    "print(ROOT_PATH)\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# os.chdir(ROOT_PATH)"
   ],
   "id": "50cc1d6452e3616c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/andrey/PycharmProjects/vector-search-hse\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T20:16:26.897152Z",
     "start_time": "2025-11-01T20:16:26.879630Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(ROOT_PATH / 'data/corpora.parquet')\n",
    "df = df[df['unique_n_tokens'] > 5]\n",
    "df"
   ],
   "id": "5122795c1e491985",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         filename                                               text  \\\n",
       "0    IMG_0703.tsv  This is the most dangerous strike in mixed mar...   \n",
       "1    IMG_0704.tsv  So this puzzle is a little harder, but let's g...   \n",
       "2    IMG_0705.tsv  I want to talk about the lie of cultural appro...   \n",
       "3    IMG_0707.tsv  I was like, oh girl, shock it, I can't let you...   \n",
       "5    IMG_0711.tsv  Oh, what the hell? We're already qualifying. W...   \n",
       "..            ...                                                ...   \n",
       "294  IMG_1916.tsv  Your favorite dinosaurs are T-Rex? Well, that'...   \n",
       "296  IMG_1923.tsv  That is why you never try to spell Mississippi...   \n",
       "297  IMG_1926.tsv  Why are there kids painting in the streets on ...   \n",
       "298  IMG_1929.tsv  and for today's experiment we're going with ba...   \n",
       "300  IMG_1931.tsv  This recipe has over 23,000 five-star reviews,...   \n",
       "\n",
       "     n_tokens  length lang     score  \\\n",
       "0         171     877   en  0.850605   \n",
       "1         142     760   en  0.981890   \n",
       "2         716    3971   en  0.983911   \n",
       "3          36     157   en  0.988990   \n",
       "5          27     142   en  0.971755   \n",
       "..        ...     ...  ...       ...   \n",
       "294       142     846   en  0.938825   \n",
       "296       157     915   en  0.944783   \n",
       "297       490    2880   en  0.960431   \n",
       "298       180     922   en  0.948105   \n",
       "300       269    1501   en  0.924810   \n",
       "\n",
       "                                     preprocessed_text  \\\n",
       "0    this is the most dangerous strike in mixed mar...   \n",
       "1    so this puzzle is little harder but let give i...   \n",
       "2    want to talk about the lie of cultural appropr...   \n",
       "3    was like oh girl shock it can let you care sav...   \n",
       "5    oh what the hell we re already qualifying what...   \n",
       "..                                                 ...   \n",
       "294  your favorite dinosaurs are rex well that nuts...   \n",
       "296  that is why you never try to spell mississippi...   \n",
       "297  why are there kids painting in the streets on ...   \n",
       "298  and for today experiment we re going with baco...   \n",
       "300  this recipe has over 23 000 five star reviews ...   \n",
       "\n",
       "                                        tokenized_text  \\\n",
       "0    [this, is, the, most, dangerous, strike, in, m...   \n",
       "1    [so, this, puzzle, is, little, harder, but, le...   \n",
       "2    [want, to, talk, about, the, lie, of, cultural...   \n",
       "3    [was, like, oh, girl, shock, it, can, let, you...   \n",
       "5    [oh, what, the, hell, we, re, already, qualify...   \n",
       "..                                                 ...   \n",
       "294  [your, favorite, dinosaurs, are, rex, well, th...   \n",
       "296  [that, is, why, you, never, try, to, spell, mi...   \n",
       "297  [why, are, there, kids, painting, in, the, str...   \n",
       "298  [and, for, today, experiment, we, re, going, w...   \n",
       "300  [this, recipe, has, over, 23, 000, five, star,...   \n",
       "\n",
       "                                       filtered_tokens  filtered_n_tokens  \\\n",
       "0    [dangerous, strike, mixed, martial, arts, hamm...                 78   \n",
       "1    [puzzle, little, harder, let, give, go, mitten...                 73   \n",
       "2    [want, talk, lie, cultural, appropriation, whi...                341   \n",
       "3    [like, oh, girl, shock, let, care, save, girl,...                 18   \n",
       "5    [oh, hell, already, qualifying, hell, hell, he...                 14   \n",
       "..                                                 ...                ...   \n",
       "294  [favorite, dinosaurs, rex, well, nuts, favorit...                 87   \n",
       "296  [never, try, spell, mississippi, mouth, full, ...                 81   \n",
       "297  [kids, painting, streets, random, weekday, sep...                246   \n",
       "298  [today, experiment, going, bacon, fat, melting...                 96   \n",
       "300  [recipe, 23, 000, five, star, reviews, takes, ...                144   \n",
       "\n",
       "                                         unique_tokens  unique_n_tokens  \\\n",
       "0    [strike, whole, moat, right, pull, shot, time,...               67   \n",
       "1    [added, let, new, two, bridge, link, issues, d...               62   \n",
       "2    [ignorant, believe, leave, much, lie, video, a...              200   \n",
       "3    [let, fall, turn, like, care, save, oh, eyes, ...               10   \n",
       "5       [already, qualifying, let, go, hell, oh, okay]                7   \n",
       "..                                                 ...              ...   \n",
       "294  [steroids, technically, let, active, bro, gays...               70   \n",
       "296  [oh, every, insignificant, woven, pistol, feel...               66   \n",
       "297  [hawthorne, explain, project, treatment, three...              197   \n",
       "298  [roasted, much, usual, gluten, yet, dough, hou...               81   \n",
       "300  [inevitable, recipe, plain, ingredients, back,...              117   \n",
       "\n",
       "     is_valid  \n",
       "0        True  \n",
       "1        True  \n",
       "2        True  \n",
       "3        True  \n",
       "5        True  \n",
       "..        ...  \n",
       "294      True  \n",
       "296      True  \n",
       "297      True  \n",
       "298      True  \n",
       "300      True  \n",
       "\n",
       "[248 rows x 13 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>text</th>\n",
       "      <th>n_tokens</th>\n",
       "      <th>length</th>\n",
       "      <th>lang</th>\n",
       "      <th>score</th>\n",
       "      <th>preprocessed_text</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>filtered_tokens</th>\n",
       "      <th>filtered_n_tokens</th>\n",
       "      <th>unique_tokens</th>\n",
       "      <th>unique_n_tokens</th>\n",
       "      <th>is_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IMG_0703.tsv</td>\n",
       "      <td>This is the most dangerous strike in mixed mar...</td>\n",
       "      <td>171</td>\n",
       "      <td>877</td>\n",
       "      <td>en</td>\n",
       "      <td>0.850605</td>\n",
       "      <td>this is the most dangerous strike in mixed mar...</td>\n",
       "      <td>[this, is, the, most, dangerous, strike, in, m...</td>\n",
       "      <td>[dangerous, strike, mixed, martial, arts, hamm...</td>\n",
       "      <td>78</td>\n",
       "      <td>[strike, whole, moat, right, pull, shot, time,...</td>\n",
       "      <td>67</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IMG_0704.tsv</td>\n",
       "      <td>So this puzzle is a little harder, but let's g...</td>\n",
       "      <td>142</td>\n",
       "      <td>760</td>\n",
       "      <td>en</td>\n",
       "      <td>0.981890</td>\n",
       "      <td>so this puzzle is little harder but let give i...</td>\n",
       "      <td>[so, this, puzzle, is, little, harder, but, le...</td>\n",
       "      <td>[puzzle, little, harder, let, give, go, mitten...</td>\n",
       "      <td>73</td>\n",
       "      <td>[added, let, new, two, bridge, link, issues, d...</td>\n",
       "      <td>62</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IMG_0705.tsv</td>\n",
       "      <td>I want to talk about the lie of cultural appro...</td>\n",
       "      <td>716</td>\n",
       "      <td>3971</td>\n",
       "      <td>en</td>\n",
       "      <td>0.983911</td>\n",
       "      <td>want to talk about the lie of cultural appropr...</td>\n",
       "      <td>[want, to, talk, about, the, lie, of, cultural...</td>\n",
       "      <td>[want, talk, lie, cultural, appropriation, whi...</td>\n",
       "      <td>341</td>\n",
       "      <td>[ignorant, believe, leave, much, lie, video, a...</td>\n",
       "      <td>200</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IMG_0707.tsv</td>\n",
       "      <td>I was like, oh girl, shock it, I can't let you...</td>\n",
       "      <td>36</td>\n",
       "      <td>157</td>\n",
       "      <td>en</td>\n",
       "      <td>0.988990</td>\n",
       "      <td>was like oh girl shock it can let you care sav...</td>\n",
       "      <td>[was, like, oh, girl, shock, it, can, let, you...</td>\n",
       "      <td>[like, oh, girl, shock, let, care, save, girl,...</td>\n",
       "      <td>18</td>\n",
       "      <td>[let, fall, turn, like, care, save, oh, eyes, ...</td>\n",
       "      <td>10</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>IMG_0711.tsv</td>\n",
       "      <td>Oh, what the hell? We're already qualifying. W...</td>\n",
       "      <td>27</td>\n",
       "      <td>142</td>\n",
       "      <td>en</td>\n",
       "      <td>0.971755</td>\n",
       "      <td>oh what the hell we re already qualifying what...</td>\n",
       "      <td>[oh, what, the, hell, we, re, already, qualify...</td>\n",
       "      <td>[oh, hell, already, qualifying, hell, hell, he...</td>\n",
       "      <td>14</td>\n",
       "      <td>[already, qualifying, let, go, hell, oh, okay]</td>\n",
       "      <td>7</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>IMG_1916.tsv</td>\n",
       "      <td>Your favorite dinosaurs are T-Rex? Well, that'...</td>\n",
       "      <td>142</td>\n",
       "      <td>846</td>\n",
       "      <td>en</td>\n",
       "      <td>0.938825</td>\n",
       "      <td>your favorite dinosaurs are rex well that nuts...</td>\n",
       "      <td>[your, favorite, dinosaurs, are, rex, well, th...</td>\n",
       "      <td>[favorite, dinosaurs, rex, well, nuts, favorit...</td>\n",
       "      <td>87</td>\n",
       "      <td>[steroids, technically, let, active, bro, gays...</td>\n",
       "      <td>70</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>IMG_1923.tsv</td>\n",
       "      <td>That is why you never try to spell Mississippi...</td>\n",
       "      <td>157</td>\n",
       "      <td>915</td>\n",
       "      <td>en</td>\n",
       "      <td>0.944783</td>\n",
       "      <td>that is why you never try to spell mississippi...</td>\n",
       "      <td>[that, is, why, you, never, try, to, spell, mi...</td>\n",
       "      <td>[never, try, spell, mississippi, mouth, full, ...</td>\n",
       "      <td>81</td>\n",
       "      <td>[oh, every, insignificant, woven, pistol, feel...</td>\n",
       "      <td>66</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>IMG_1926.tsv</td>\n",
       "      <td>Why are there kids painting in the streets on ...</td>\n",
       "      <td>490</td>\n",
       "      <td>2880</td>\n",
       "      <td>en</td>\n",
       "      <td>0.960431</td>\n",
       "      <td>why are there kids painting in the streets on ...</td>\n",
       "      <td>[why, are, there, kids, painting, in, the, str...</td>\n",
       "      <td>[kids, painting, streets, random, weekday, sep...</td>\n",
       "      <td>246</td>\n",
       "      <td>[hawthorne, explain, project, treatment, three...</td>\n",
       "      <td>197</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>IMG_1929.tsv</td>\n",
       "      <td>and for today's experiment we're going with ba...</td>\n",
       "      <td>180</td>\n",
       "      <td>922</td>\n",
       "      <td>en</td>\n",
       "      <td>0.948105</td>\n",
       "      <td>and for today experiment we re going with baco...</td>\n",
       "      <td>[and, for, today, experiment, we, re, going, w...</td>\n",
       "      <td>[today, experiment, going, bacon, fat, melting...</td>\n",
       "      <td>96</td>\n",
       "      <td>[roasted, much, usual, gluten, yet, dough, hou...</td>\n",
       "      <td>81</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>IMG_1931.tsv</td>\n",
       "      <td>This recipe has over 23,000 five-star reviews,...</td>\n",
       "      <td>269</td>\n",
       "      <td>1501</td>\n",
       "      <td>en</td>\n",
       "      <td>0.924810</td>\n",
       "      <td>this recipe has over 23 000 five star reviews ...</td>\n",
       "      <td>[this, recipe, has, over, 23, 000, five, star,...</td>\n",
       "      <td>[recipe, 23, 000, five, star, reviews, takes, ...</td>\n",
       "      <td>144</td>\n",
       "      <td>[inevitable, recipe, plain, ingredients, back,...</td>\n",
       "      <td>117</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>248 rows × 13 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Базовое представление",
   "id": "c0dc5a3fd048d9a4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T20:16:26.901090Z",
     "start_time": "2025-11-01T20:16:26.898168Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from bertopic import BERTopic\n",
    "texts = [' '.join(tokens) for tokens in df['filtered_tokens'].values]\n",
    "len(texts)"
   ],
   "id": "61839472a29828c6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "248"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T20:16:56.715097Z",
     "start_time": "2025-11-01T20:16:53.181263Z"
    }
   },
   "cell_type": "code",
   "source": [
    "topic_model = BERTopic()\n",
    "topics, probs = topic_model.fit_transform(texts)"
   ],
   "id": "c0c82fd6cec67f93",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T20:16:56.720695Z",
     "start_time": "2025-11-01T20:16:56.715838Z"
    }
   },
   "cell_type": "code",
   "source": "topic_model.get_topic_info()",
   "id": "1a41de9d99cea67d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   Topic  Count                      Name  \\\n",
       "0      0    190     0_like_know_one_going   \n",
       "1      1     58  1_chicken_going_like_add   \n",
       "\n",
       "                                      Representation  \\\n",
       "0  [like, know, one, going, yeah, oh, right, want...   \n",
       "1  [chicken, going, like, add, get, one, butter, ...   \n",
       "\n",
       "                                 Representative_Docs  \n",
       "0  [brains hardwired panic face looks almost huma...  \n",
       "1  [bleh want dinner hangover chicken nuggets gon...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>190</td>\n",
       "      <td>0_like_know_one_going</td>\n",
       "      <td>[like, know, one, going, yeah, oh, right, want...</td>\n",
       "      <td>[brains hardwired panic face looks almost huma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>1_chicken_going_like_add</td>\n",
       "      <td>[chicken, going, like, add, get, one, butter, ...</td>\n",
       "      <td>[bleh want dinner hangover chicken nuggets gon...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T20:16:56.819178Z",
     "start_time": "2025-11-01T20:16:56.721419Z"
    }
   },
   "cell_type": "code",
   "source": "topic_model.visualize_topics()",
   "id": "958044c71acd258f",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "zero-size array to reduction operation maximum which has no identity",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[15]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[43mtopic_model\u001B[49m\u001B[43m.\u001B[49m\u001B[43mvisualize_topics\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/vector-search-hse/.venv/lib/python3.12/site-packages/bertopic/_bertopic.py:2442\u001B[39m, in \u001B[36mBERTopic.visualize_topics\u001B[39m\u001B[34m(self, topics, top_n_topics, use_ctfidf, custom_labels, title, width, height)\u001B[39m\n\u001B[32m   2409\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Visualize topics, their sizes, and their corresponding words.\u001B[39;00m\n\u001B[32m   2410\u001B[39m \n\u001B[32m   2411\u001B[39m \u001B[33;03mThis visualization is highly inspired by LDAvis, a great visualization\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   2439\u001B[39m \u001B[33;03m```\u001B[39;00m\n\u001B[32m   2440\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   2441\u001B[39m check_is_fitted(\u001B[38;5;28mself\u001B[39m)\n\u001B[32m-> \u001B[39m\u001B[32m2442\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mplotting\u001B[49m\u001B[43m.\u001B[49m\u001B[43mvisualize_topics\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2443\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m   2444\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtopics\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtopics\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2445\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtop_n_topics\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtop_n_topics\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2446\u001B[39m \u001B[43m    \u001B[49m\u001B[43muse_ctfidf\u001B[49m\u001B[43m=\u001B[49m\u001B[43muse_ctfidf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2447\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcustom_labels\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcustom_labels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2448\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtitle\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtitle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2449\u001B[39m \u001B[43m    \u001B[49m\u001B[43mwidth\u001B[49m\u001B[43m=\u001B[49m\u001B[43mwidth\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2450\u001B[39m \u001B[43m    \u001B[49m\u001B[43mheight\u001B[49m\u001B[43m=\u001B[49m\u001B[43mheight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2451\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/vector-search-hse/.venv/lib/python3.12/site-packages/bertopic/plotting/_topics.py:102\u001B[39m, in \u001B[36mvisualize_topics\u001B[39m\u001B[34m(topic_model, topics, top_n_topics, use_ctfidf, custom_labels, title, width, height)\u001B[39m\n\u001B[32m     98\u001B[39m         embeddings = UMAP(n_neighbors=\u001B[32m2\u001B[39m, n_components=\u001B[32m2\u001B[39m, metric=\u001B[33m\"\u001B[39m\u001B[33mhellinger\u001B[39m\u001B[33m\"\u001B[39m, random_state=\u001B[32m42\u001B[39m).fit_transform(\n\u001B[32m     99\u001B[39m             embeddings\n\u001B[32m    100\u001B[39m         )\n\u001B[32m    101\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m102\u001B[39m         embeddings = \u001B[43mUMAP\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn_neighbors\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_components\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetric\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcosine\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m42\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfit_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43membeddings\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    103\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    104\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mModuleNotFoundError\u001B[39;00m(\n\u001B[32m    105\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mUMAP is required to reduce the embeddings.. Please install it using `pip install umap-learn`.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    106\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/vector-search-hse/.venv/lib/python3.12/site-packages/umap/umap_.py:2935\u001B[39m, in \u001B[36mUMAP.fit_transform\u001B[39m\u001B[34m(self, X, y, ensure_all_finite, **kwargs)\u001B[39m\n\u001B[32m   2897\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mfit_transform\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y=\u001B[38;5;28;01mNone\u001B[39;00m, ensure_all_finite=\u001B[38;5;28;01mTrue\u001B[39;00m, **kwargs):\n\u001B[32m   2898\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Fit X into an embedded space and return that transformed\u001B[39;00m\n\u001B[32m   2899\u001B[39m \u001B[33;03m    output.\u001B[39;00m\n\u001B[32m   2900\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m   2933\u001B[39m \u001B[33;03m        Local radii of data points in the embedding (log-transformed).\u001B[39;00m\n\u001B[32m   2934\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m2935\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mensure_all_finite\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2936\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.transform_mode == \u001B[33m\"\u001B[39m\u001B[33membedding\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m   2937\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.output_dens:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/vector-search-hse/.venv/lib/python3.12/site-packages/umap/umap_.py:2817\u001B[39m, in \u001B[36mUMAP.fit\u001B[39m\u001B[34m(self, X, y, ensure_all_finite, **kwargs)\u001B[39m\n\u001B[32m   2813\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.transform_mode == \u001B[33m\"\u001B[39m\u001B[33membedding\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m   2814\u001B[39m     epochs = (\n\u001B[32m   2815\u001B[39m         \u001B[38;5;28mself\u001B[39m.n_epochs_list \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.n_epochs_list \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m.n_epochs\n\u001B[32m   2816\u001B[39m     )\n\u001B[32m-> \u001B[39m\u001B[32m2817\u001B[39m     \u001B[38;5;28mself\u001B[39m.embedding_, aux_data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_fit_embed_data\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2818\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_raw_data\u001B[49m\u001B[43m[\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2819\u001B[39m \u001B[43m        \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2820\u001B[39m \u001B[43m        \u001B[49m\u001B[43minit\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2821\u001B[39m \u001B[43m        \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# JH why raw data?\u001B[39;49;00m\n\u001B[32m   2822\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2823\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2825\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.n_epochs_list \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   2826\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33membedding_list\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m aux_data:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/vector-search-hse/.venv/lib/python3.12/site-packages/umap/umap_.py:2872\u001B[39m, in \u001B[36mUMAP._fit_embed_data\u001B[39m\u001B[34m(self, X, n_epochs, init, random_state, **kwargs)\u001B[39m\n\u001B[32m   2867\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_fit_embed_data\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, n_epochs, init, random_state, **kwargs):\n\u001B[32m   2868\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"A method wrapper for simplicial_set_embedding that can be\u001B[39;00m\n\u001B[32m   2869\u001B[39m \u001B[33;03m    replaced by subclasses. Arbitrary keyword arguments can be passed\u001B[39;00m\n\u001B[32m   2870\u001B[39m \u001B[33;03m    through .fit() and .fit_transform().\u001B[39;00m\n\u001B[32m   2871\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m2872\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43msimplicial_set_embedding\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2873\u001B[39m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2874\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgraph_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2875\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mn_components\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2876\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_initial_alpha\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2877\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_a\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2878\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_b\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2879\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mrepulsion_strength\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2880\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mnegative_sample_rate\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2881\u001B[39m \u001B[43m        \u001B[49m\u001B[43mn_epochs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2882\u001B[39m \u001B[43m        \u001B[49m\u001B[43minit\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2883\u001B[39m \u001B[43m        \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2884\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_input_distance_func\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2885\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_metric_kwds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2886\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdensmap\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2887\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_densmap_kwds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2888\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moutput_dens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2889\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_output_distance_func\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2890\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_output_metric_kwds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2891\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moutput_metric\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43meuclidean\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43ml2\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2892\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m   2893\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2894\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtqdm_kwds\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtqdm_kwds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2895\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/vector-search-hse/.venv/lib/python3.12/site-packages/umap/umap_.py:1089\u001B[39m, in \u001B[36msimplicial_set_embedding\u001B[39m\u001B[34m(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, densmap, densmap_kwds, output_dens, output_metric, output_metric_kwds, euclidean_output, parallel, verbose, tqdm_kwds)\u001B[39m\n\u001B[32m   1086\u001B[39m n_epochs_max = \u001B[38;5;28mmax\u001B[39m(n_epochs) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(n_epochs, \u001B[38;5;28mlist\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m n_epochs\n\u001B[32m   1088\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m n_epochs_max > \u001B[32m10\u001B[39m:\n\u001B[32m-> \u001B[39m\u001B[32m1089\u001B[39m     graph.data[graph.data < (\u001B[43mgraph\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmax\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m / \u001B[38;5;28mfloat\u001B[39m(n_epochs_max))] = \u001B[32m0.0\u001B[39m\n\u001B[32m   1090\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1091\u001B[39m     graph.data[graph.data < (graph.data.max() / \u001B[38;5;28mfloat\u001B[39m(default_epochs))] = \u001B[32m0.0\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/vector-search-hse/.venv/lib/python3.12/site-packages/numpy/_core/_methods.py:43\u001B[39m, in \u001B[36m_amax\u001B[39m\u001B[34m(a, axis, out, keepdims, initial, where)\u001B[39m\n\u001B[32m     41\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_amax\u001B[39m(a, axis=\u001B[38;5;28;01mNone\u001B[39;00m, out=\u001B[38;5;28;01mNone\u001B[39;00m, keepdims=\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[32m     42\u001B[39m           initial=_NoValue, where=\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[32m---> \u001B[39m\u001B[32m43\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mumr_maximum\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkeepdims\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minitial\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwhere\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mValueError\u001B[39m: zero-size array to reduction operation maximum which has no identity"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Tuning best\n",
    "\n",
    " Моделирование с помощью Bertopic позволяет нам использоваться кастомные модули для работы с нашими документа и конвейром\n",
    " \n",
    "1. Задействуем эмбеддинги с LLM\n",
    "2. Задействуем снижение размерностей с помощью Umap\n",
    "3. Для кластеринга будет использоваться HDBSCAN"
   ],
   "id": "c4dfdaa4bd53ff69"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T20:20:39.571630Z",
     "start_time": "2025-11-01T20:20:36.323105Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Pre-calculate embeddings\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = embedding_model.encode(texts, show_progress_bar=True)"
   ],
   "id": "7275ba3eb431974d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 8/8 [00:00<00:00,  9.88it/s]\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T20:21:05.566633Z",
     "start_time": "2025-11-01T20:21:05.564641Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from umap import UMAP\n",
    "\n",
    "umap_model = UMAP(n_neighbors=5, n_components=40, min_dist=0.0, metric='cosine', random_state=42)"
   ],
   "id": "93737a804611f25f",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T20:21:05.756224Z",
     "start_time": "2025-11-01T20:21:05.754394Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from hdbscan import HDBSCAN\n",
    "\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=5, metric='euclidean', cluster_selection_method='eom', prediction_data=True)"
   ],
   "id": "2668d83ad9ca67d0",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T20:21:05.935499Z",
     "start_time": "2025-11-01T20:21:05.933465Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer_model = CountVectorizer(stop_words='english', min_df=2, max_df=100, ngram_range=(1, 2))"
   ],
   "id": "e854cfd7fb38627e",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T20:21:06.114700Z",
     "start_time": "2025-11-01T20:21:06.113086Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance, PartOfSpeech\n",
    "\n",
    "# KeyBERT\n",
    "keybert_model = KeyBERTInspired()\n",
    "\n",
    "# MMR\n",
    "mmr_model = MaximalMarginalRelevance(diversity=0.1)\n",
    "\n",
    "# All representation models\n",
    "representation_model = {\n",
    "    \"KeyBERT\": keybert_model,\n",
    "    \"MMR\": mmr_model,\n",
    "}"
   ],
   "id": "661f9288b109a4eb",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T20:21:42.552914Z",
     "start_time": "2025-11-01T20:21:41.827413Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from bertopic import BERTopic\n",
    "\n",
    "\n",
    "zeroshot_topics = [\n",
    "    'guide',\n",
    "    'cooking',\n",
    "    'minecraft',\n",
    "    'humour'\n",
    "]\n",
    "\n",
    "\n",
    "topic_model = BERTopic(\n",
    "  # Pipeline models\n",
    "    zeroshot_topic_list=zeroshot_topics,\n",
    "    embedding_model=embedding_model,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    representation_model=representation_model,\n",
    "\n",
    "  # Hyperparameters\n",
    "  top_n_words=10,\n",
    "  verbose=True\n",
    ")\n",
    "\n",
    "# Train model\n",
    "topics, probs = topic_model.fit_transform(texts, embeddings)\n",
    "\n",
    "# Show topics\n",
    "topic_model.get_topic_info()"
   ],
   "id": "7169b669075c233f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-01 23:21:41,828 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-11-01 23:21:42,038 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-11-01 23:21:42,039 - BERTopic - Zeroshot Step 1 - Finding documents that could be assigned to either one of the zero-shot topics\n",
      "2025-11-01 23:21:42,314 - BERTopic - Zeroshot Step 1 - Completed ✓\n",
      "2025-11-01 23:21:42,315 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-11-01 23:21:42,320 - BERTopic - Cluster - Completed ✓\n",
      "2025-11-01 23:21:42,321 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-11-01 23:21:42,528 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   Topic  Count                        Name  \\\n",
       "0      0    185      0_like_know_yeah_going   \n",
       "1      1     52    1_chicken_going_add_like   \n",
       "2      2     11  2_cells_cell_water_plastic   \n",
       "\n",
       "                                      Representation  \\\n",
       "0  [like, know, yeah, going, oh, want, think, rig...   \n",
       "1  [chicken, going, add, like, butter, try, water...   \n",
       "2  [cells, cell, water, plastic, really, cancer, ...   \n",
       "\n",
       "                                             KeyBERT  \\\n",
       "0  [like, say, mean, think, care, idea, goes, mic...   \n",
       "1  [recipe, cooking, cook, chicken, meal, cooked,...   \n",
       "2  [biological, cells, genetic, cell, fuel, scien...   \n",
       "\n",
       "                                                 MMR  \\\n",
       "0  [like, yeah, going, want, think, time, love, s...   \n",
       "1  [chicken, butter, milk, cook, make, chocolate,...   \n",
       "2  [cells, cell, water, plastic, cancer, fuel, sc...   \n",
       "\n",
       "                                 Representative_Docs  \n",
       "0  [right fucking talk something literally talkin...  \n",
       "1  [bleh want dinner hangover chicken nuggets gon...  \n",
       "2  [remember important scientific paper 2025 vide...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>KeyBERT</th>\n",
       "      <th>MMR</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>185</td>\n",
       "      <td>0_like_know_yeah_going</td>\n",
       "      <td>[like, know, yeah, going, oh, want, think, rig...</td>\n",
       "      <td>[like, say, mean, think, care, idea, goes, mic...</td>\n",
       "      <td>[like, yeah, going, want, think, time, love, s...</td>\n",
       "      <td>[right fucking talk something literally talkin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>1_chicken_going_add_like</td>\n",
       "      <td>[chicken, going, add, like, butter, try, water...</td>\n",
       "      <td>[recipe, cooking, cook, chicken, meal, cooked,...</td>\n",
       "      <td>[chicken, butter, milk, cook, make, chocolate,...</td>\n",
       "      <td>[bleh want dinner hangover chicken nuggets gon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>2_cells_cell_water_plastic</td>\n",
       "      <td>[cells, cell, water, plastic, really, cancer, ...</td>\n",
       "      <td>[biological, cells, genetic, cell, fuel, scien...</td>\n",
       "      <td>[cells, cell, water, plastic, cancer, fuel, sc...</td>\n",
       "      <td>[remember important scientific paper 2025 vide...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "83fc9a57c10d8cc9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
